{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Website Ingestion Pipeline","text":"<p>A modular web crawling and data extraction pipeline that crawls websites using breadth-first search, extracts structured content, and exports datasets in multiple formats.</p>"},{"location":"#overview","title":"Overview","text":"<p>The pipeline provides two interfaces \u2014 a command-line tool and a REST API \u2014 for crawling websites and producing structured, analysis-ready datasets. It handles the full lifecycle: URL discovery, content fetching, text and image extraction, and export to Parquet, CSV, or JSONL.</p> <p>Key capabilities:</p> <ul> <li>Breadth-first crawl engine with configurable depth, page limits, and rate limiting</li> <li>Robots.txt compliance with crawl-delay support</li> <li>Content extraction covering titles, headings, meta descriptions, body text, images, and links</li> <li>Dynamic rendering via Playwright/Chromium for JavaScript-heavy pages</li> <li>Multiple export formats \u2014 Parquet, CSV, and JSONL</li> <li>Raw HTML archival with gzip compression and metadata sidecars</li> <li>Image downloading with async fetching and perceptual deduplication</li> <li>Human-readable Markdown exports alongside structured data</li> <li>REST API for submitting and managing crawl jobs over HTTP</li> <li>Docker deployment with a single command</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#using-docker","title":"Using Docker","text":"<pre><code>docker compose up -d\n</code></pre> <p>The API server starts on <code>http://localhost:8000</code>. Submit a crawl job:</p> <pre><code>curl -X POST http://localhost:8000/api/crawl \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com\", \"max_depth\": 2, \"page_limit\": 50}'\n</code></pre>"},{"location":"#using-the-cli","title":"Using the CLI","text":"<pre><code>docker compose exec pipeline-api /entrypoint.sh crawl \\\n  --url https://example.com --depth 2 --limit 50\n</code></pre>"},{"location":"#running-locally","title":"Running Locally","text":"<pre><code>pip install -r requirements.txt\nplaywright install chromium\npython main.py --url https://example.com\n</code></pre> <p>See Getting Started for full installation and setup instructions.</p>"},{"location":"#documentation","title":"Documentation","text":"Section Description Getting Started Installation, setup, and first crawl walkthrough CLI Reference Complete command-line option documentation API Reference REST API endpoints, schemas, and examples Output Format Dataset structure and field definitions Architecture System design, module overview, and configuration"},{"location":"api-reference/","title":"API Reference","text":"<p>The REST API runs on port <code>8000</code> by default. Interactive Swagger documentation is available at <code>/docs</code> when the server is running.</p> <p>Base URL: <code>http://localhost:8000</code></p>"},{"location":"api-reference/#health-check","title":"Health Check","text":"<p>Check that the service is running.</p> <pre><code>GET /\n</code></pre> <p>Response:</p> <pre><code>{\n  \"service\": \"Website Ingestion Pipeline\",\n  \"status\": \"running\",\n  \"version\": \"1.0.0\",\n  \"docs\": \"/docs\"\n}\n</code></pre>"},{"location":"api-reference/#submit-a-crawl-job","title":"Submit a Crawl Job","text":"<p>Start a new crawl. The request returns immediately with a job ID; the crawl runs in the background.</p> <pre><code>POST /api/crawl\n</code></pre> <p>Request body:</p> Field Type Default Description <code>url</code> string (required) Starting URL. Must use <code>http://</code> or <code>https://</code>. <code>max_depth</code> integer <code>3</code> Maximum BFS crawl depth (1--20). <code>page_limit</code> integer <code>100</code> Maximum pages to crawl (1--10,000). <code>rate_limit</code> float <code>1.0</code> Seconds between requests. <code>output_format</code> string <code>\"parquet\"</code> Export format: <code>parquet</code>, <code>csv</code>, or <code>jsonl</code>. <code>save_raw_html</code> boolean <code>true</code> Save gzip-compressed raw HTML. <code>download_images</code> boolean <code>true</code> Download images from crawled pages. <code>dynamic_fallback</code> boolean <code>true</code> Use Playwright for JavaScript-heavy pages. <p>Example:</p> <pre><code>curl -X POST http://localhost:8000/api/crawl \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"https://example.com\",\n    \"max_depth\": 2,\n    \"page_limit\": 50,\n    \"rate_limit\": 1.0,\n    \"output_format\": \"parquet\"\n  }'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"job_id\": \"a1b2c3d4\",\n  \"status\": \"queued\",\n  \"message\": \"Crawl job submitted. Poll /api/jobs/a1b2c3d4 for status.\"\n}\n</code></pre>"},{"location":"api-reference/#list-jobs","title":"List Jobs","text":"<p>List all crawl jobs, optionally filtered by status.</p> <pre><code>GET /api/jobs\nGET /api/jobs?status=completed\n</code></pre> <p>Query parameters:</p> Parameter Type Description <code>status</code> string Filter by job status: <code>queued</code>, <code>running</code>, <code>completed</code>, <code>failed</code>. Optional. <p>Response:</p> <pre><code>[\n  {\n    \"job_id\": \"a1b2c3d4\",\n    \"status\": \"completed\",\n    \"url\": \"https://example.com\",\n    \"created_at\": \"2026-02-23T10:00:00Z\",\n    \"completed_at\": \"2026-02-23T10:01:30Z\",\n    \"report\": { ... },\n    \"error\": null\n  }\n]\n</code></pre> <p>Jobs are sorted by creation time, most recent first.</p>"},{"location":"api-reference/#get-job-status","title":"Get Job Status","text":"<p>Retrieve the status and report for a specific job.</p> <pre><code>GET /api/jobs/{job_id}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"job_id\": \"a1b2c3d4\",\n  \"status\": \"completed\",\n  \"url\": \"https://example.com\",\n  \"created_at\": \"2026-02-23T10:00:00Z\",\n  \"completed_at\": \"2026-02-23T10:01:30Z\",\n  \"report\": {\n    \"total_pages\": 15,\n    \"failed_pages\": 0,\n    \"total_images\": 42,\n    \"external_links\": 8,\n    \"time_taken_seconds\": 12.5,\n    \"errors\": []\n  },\n  \"error\": null\n}\n</code></pre> <p>Status values:</p> Status Description <code>queued</code> Job submitted, waiting to start <code>running</code> Crawl is in progress <code>completed</code> Crawl finished successfully <code>failed</code> Crawl terminated with an error"},{"location":"api-reference/#download-crawl-report","title":"Download Crawl Report","text":"<p>Download the <code>crawl_report.json</code> file for a completed job.</p> <pre><code>GET /api/jobs/{job_id}/report\n</code></pre> <p>Returns a JSON file download. Only available for completed jobs.</p>"},{"location":"api-reference/#download-pages-dataset","title":"Download Pages Dataset","text":"<p>Download the pages dataset for a completed job.</p> <pre><code>GET /api/jobs/{job_id}/pages\n</code></pre> <p>Returns the <code>pages.parquet</code>, <code>pages.csv</code>, or <code>pages.jsonl</code> file, depending on the format used when the job was submitted.</p>"},{"location":"api-reference/#download-images-dataset","title":"Download Images Dataset","text":"<p>Download the images metadata dataset for a completed job.</p> <pre><code>GET /api/jobs/{job_id}/images\n</code></pre> <p>Returns the <code>images.parquet</code>, <code>images.csv</code>, or <code>images.jsonl</code> file.</p>"},{"location":"api-reference/#download-full-dataset","title":"Download Full Dataset","text":"<p>Download the entire output directory as a ZIP archive.</p> <pre><code>GET /api/jobs/{job_id}/download\n</code></pre> <p>The archive is created on demand if it does not already exist. Contains all output files: datasets, report, manifest, readable Markdown, raw HTML, and images.</p>"},{"location":"api-reference/#delete-a-job","title":"Delete a Job","text":"<p>Remove a job and delete all associated files from disk.</p> <pre><code>DELETE /api/jobs/{job_id}\n</code></pre> <p>Running jobs cannot be deleted. Returns:</p> <pre><code>{\n  \"message\": \"Job 'a1b2c3d4' deleted.\"\n}\n</code></pre>"},{"location":"api-reference/#error-responses","title":"Error Responses","text":"<p>All error responses use the standard format:</p> <pre><code>{\n  \"detail\": \"Description of the error.\"\n}\n</code></pre> Status Code Meaning <code>400</code> Bad request (invalid input, job not in correct state) <code>404</code> Job or file not found <code>422</code> Validation error (malformed request body)"},{"location":"architecture/","title":"Architecture","text":"<p>This page describes the internal design of the pipeline, the role of each module, and the Docker deployment configuration.</p>"},{"location":"architecture/#pipeline-flow","title":"Pipeline Flow","text":"<p>The crawl lifecycle follows a linear pipeline from URL discovery through data export.</p> <pre><code>flowchart TD\n    Seed[\"Seed URL\"] --&gt; Frontier[\"URL Frontier\"]\n    Frontier --&gt; RobotsCheck[\"Robots.txt Check\"]\n    RobotsCheck --&gt;|allowed| Fetcher[\"Page Fetcher\"]\n    RobotsCheck --&gt;|blocked| Skip[\"Skip URL\"]\n    Fetcher --&gt;|static| Parser[\"HTML Parser\"]\n    Fetcher --&gt;|\"JS-heavy (fallback)\"| Playwright[\"Playwright Renderer\"]\n    Playwright --&gt; Parser\n    Parser --&gt; Extractors[\"Extractor Pipeline\"]\n    Extractors --&gt; TextExtractor[\"Text Extractor\"]\n    Extractors --&gt; ImageExtractor[\"Image Extractor\"]\n    Extractors --&gt; LinkExtractor[\"Link Extractor\"]\n    TextExtractor --&gt; Structurer[\"Data Structurer\"]\n    ImageExtractor --&gt; Structurer\n    LinkExtractor --&gt;|internal| Frontier\n    LinkExtractor --&gt;|external| Structurer\n    Structurer --&gt; Export[\"Export\"]\n    Export --&gt; Parquet[\"Parquet / CSV / JSONL\"]\n    Export --&gt; Readable[\"Markdown\"]\n    Export --&gt; Report[\"Crawl Report\"]\n    Export --&gt; Manifest[\"Manifest\"]</code></pre> <ol> <li>The seed URL is added to the frontier at depth 0.</li> <li>The frontier manages the BFS queue, normalizes URLs, filters by domain, and prevents revisits.</li> <li>Robots.txt rules are checked per-domain (cached). Blocked URLs are skipped.</li> <li>The fetcher makes an async HTTP request. If the response appears JavaScript-heavy (low text-to-HTML ratio), it falls back to Playwright/Chromium.</li> <li>The parser strips scripts, styles, and non-content elements, then uses <code>readability-lxml</code> to extract the main content.</li> <li>The extractor pipeline runs three extractors in sequence: text (title, headings, meta, body), images (download and deduplicate), and links (classify internal vs. external, filter noise).</li> <li>Internal links are enqueued back into the frontier at <code>depth + 1</code>.</li> <li>All extracted data accumulates in the structurer, which exports datasets at the end of the crawl.</li> </ol>"},{"location":"architecture/#module-overview","title":"Module Overview","text":"<pre><code>main.py                         Entry point (CLI + API router)\napp/\n\u251c\u2500\u2500 api.py                      FastAPI REST interface\n\u251c\u2500\u2500 job_manager.py              Job orchestration and lifecycle\n\u251c\u2500\u2500 crawler.py                  BFS crawl loop\n\u251c\u2500\u2500 fetcher.py                  Async HTTP client + Playwright fallback\n\u251c\u2500\u2500 frontier.py                 URL queue, normalization, deduplication\n\u251c\u2500\u2500 robots.py                   Robots.txt fetching and rule checking\n\u251c\u2500\u2500 parser.py                   HTML parsing with readability-lxml\n\u251c\u2500\u2500 structurer.py               Data aggregation and dataset export\n\u251c\u2500\u2500 raw_storage.py              Gzip HTML archival with metadata\n\u251c\u2500\u2500 dataset_storage.py          Manifest creation and ZIP packaging\n\u251c\u2500\u2500 config.py                   Pydantic configuration models\n\u251c\u2500\u2500 logger.py                   Structured logging via structlog\n\u2514\u2500\u2500 extractors/\n    \u251c\u2500\u2500 __init__.py             Pipeline orchestrator\n    \u251c\u2500\u2500 text.py                 Title, headings, meta extraction\n    \u251c\u2500\u2500 link.py                 Link classification and filtering\n    \u2514\u2500\u2500 image.py                Image download and deduplication\n</code></pre>"},{"location":"architecture/#core-modules","title":"Core Modules","text":"<p><code>job_manager.py</code> -- Validates inputs, builds a <code>CrawlConfig</code>, creates the output directory, launches the crawler, triggers export, and creates the manifest. This is the top-level orchestrator called by both the CLI and API.</p> <p><code>crawler.py</code> -- Implements the BFS loop. Pops URLs from the frontier, checks robots.txt, fetches, parses, extracts, and enqueues discovered links. Tracks progress with a Rich progress bar.</p> <p><code>fetcher.py</code> -- Async HTTP client using <code>aiohttp</code>. Supports user-agent rotation, rate limiting (enforced via semaphore and sleep), and retries with exponential backoff. Detects JavaScript-heavy pages by heuristic and falls back to Playwright/Chromium.</p> <p><code>frontier.py</code> -- Priority queue (heapq) ordered by depth for BFS traversal. Normalizes URLs (lowercase host, strip fragments, remove tracking parameters like <code>utm_*</code>, <code>fbclid</code>, <code>gclid</code>). Maintains a visited set using SHA-256 hashes.</p> <p><code>parser.py</code> -- Strips non-content tags (script, style, noscript, iframe, svg). Detects encoding with <code>chardet</code>. Uses <code>readability-lxml</code> for main content extraction, with a fallback to the full body if readability is too aggressive.</p> <p><code>structurer.py</code> -- Accumulates page records, image metadata, external links, and errors during the crawl. At export time, builds Pandas DataFrames and writes to Parquet, CSV, or JSONL. Also generates the crawl report JSON and human-readable Markdown exports.</p>"},{"location":"architecture/#extractors","title":"Extractors","text":"<p><code>text.py</code> -- Extracts the page title (from <code>&lt;title&gt;</code> or first <code>&lt;h1&gt;</code>), all headings (h1--h6 with levels), and the meta description (from <code>&lt;meta name=\"description\"&gt;</code> or <code>og:description</code>).</p> <p><code>link.py</code> -- Resolves relative URLs, classifies links as internal or external based on domain, and filters noise (login pages, admin paths, social media, mailto/tel links, binary files).</p> <p><code>image.py</code> -- Finds images via <code>&lt;img&gt;</code> tags (including <code>data-src</code> and <code>data-lazy-src</code> attributes). Downloads asynchronously, filters by minimum dimension, deduplicates using perceptual hashing (<code>imagehash</code>), and stores files named by SHA-256 content hash.</p>"},{"location":"architecture/#docker-configuration","title":"Docker Configuration","text":""},{"location":"architecture/#container","title":"Container","text":"<p>The Dockerfile uses a multi-stage build on <code>python:3.11-slim</code>:</p> <ol> <li>Builder stage -- Installs build dependencies and compiles Python packages.</li> <li>Runtime stage -- Copies compiled packages, installs runtime system libraries (libxml2, libjpeg, libpng, Chromium dependencies, fonts), and installs the Playwright Chromium browser.</li> </ol> <p>The container exposes port <code>8000</code> and uses <code>/entrypoint.sh</code> as its entrypoint.</p>"},{"location":"architecture/#docker-compose","title":"Docker Compose","text":"<p>The <code>docker-compose.yml</code> defines a single service:</p> Setting Value Container name <code>web-crawler-api</code> Default command <code>serve</code> Port mapping <code>8000:8000</code> Memory limit 2 GB (512 MB reserved) Restart policy <code>unless-stopped</code> Health check HTTP request to <code>localhost:8000</code> every 30 seconds"},{"location":"architecture/#volumes","title":"Volumes","text":"Mount Container Path Purpose <code>crawler_data</code> (named volume) <code>/app/jobs</code> Persistent storage for API job data Host Downloads folder <code>/app/downloads</code> Bind mount for CLI crawl output"},{"location":"architecture/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>PIPELINE_DATA_DIR</code> <code>/app/jobs</code> Base directory for API job storage <code>PORT</code> <code>8000</code> API server port <code>WORKERS</code> <code>1</code> Number of Uvicorn worker processes <code>PYTHONUNBUFFERED</code> <code>1</code> Disable Python output buffering <code>PURUCRAWLER_DOWNLOADS</code> (user-specific) Host path for the Downloads bind mount"},{"location":"architecture/#dependencies","title":"Dependencies","text":"Package Role aiohttp Async HTTP client for page fetching beautifulsoup4 + lxml HTML parsing and DOM traversal readability-lxml Main content extraction from HTML pandas + pyarrow DataFrame operations and Parquet export Pillow + imagehash Image processing and perceptual deduplication playwright Headless Chromium for dynamic page rendering fastapi + uvicorn REST API server typer + rich CLI framework and terminal formatting structlog Structured, key-value logging pydantic Configuration validation and serialization chardet Character encoding detection"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>The pipeline provides two commands: <code>crawl</code> (default) for running a crawl, and <code>serve</code> for starting the REST API server.</p>"},{"location":"cli-reference/#crawl-command","title":"Crawl Command","text":"<p>Crawl a website and export structured datasets.</p> <pre><code>python main.py --url &lt;URL&gt; [OPTIONS]\n</code></pre> <p>When running inside Docker:</p> <pre><code>docker compose exec pipeline-api /entrypoint.sh crawl --url &lt;URL&gt; [OPTIONS]\n</code></pre>"},{"location":"cli-reference/#options","title":"Options","text":"Option Short Default Description <code>--url</code> <code>-u</code> (required) Starting URL to crawl. Must begin with <code>http://</code> or <code>https://</code>. <code>--depth</code> <code>-d</code> <code>3</code> Maximum crawl depth. The crawler uses breadth-first search; this controls how many link-hops from the seed URL are followed. Range: 1--20. <code>--limit</code> <code>-l</code> <code>100</code> Maximum number of pages to crawl. The crawler stops after processing this many pages, even if the frontier is not exhausted. Range: 1--10,000. <code>--rate-limit</code> <code>-r</code> <code>1.0</code> Minimum delay in seconds between consecutive HTTP requests. Helps avoid overloading target servers. <code>--output-dir</code> <code>-o</code> <code>site_dataset</code> Directory where output files are written. Created automatically if it does not exist. <code>--format</code> <code>-f</code> <code>parquet</code> Export format for tabular data. Accepted values: <code>parquet</code>, <code>csv</code>, <code>jsonl</code>. <code>--no-raw-html</code> <code>false</code> Skip saving gzip-compressed raw HTML files. Reduces disk usage. <code>--no-images</code> <code>false</code> Skip downloading images from crawled pages. <code>--no-dynamic</code> <code>false</code> Disable the Playwright/Chromium fallback for JavaScript-heavy pages. Use this if Playwright is not installed or not needed. <code>--log-level</code> <code>INFO</code> Logging verbosity. Accepted values: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>. <code>--json-logs</code> <code>false</code> Output structured JSON log lines instead of human-readable console output. Useful for log aggregation systems."},{"location":"cli-reference/#examples","title":"Examples","text":"<p>Basic crawl with defaults:</p> <pre><code>python main.py --url https://example.com\n</code></pre> <p>Crawl with limited scope and CSV output:</p> <pre><code>python main.py --url https://example.com --depth 1 --limit 10 --format csv\n</code></pre> <p>Fast crawl without images or raw HTML:</p> <pre><code>python main.py --url https://example.com --no-images --no-raw-html --rate-limit 0.5\n</code></pre> <p>Verbose debug logging:</p> <pre><code>python main.py --url https://example.com --log-level DEBUG\n</code></pre>"},{"location":"cli-reference/#serve-command","title":"Serve Command","text":"<p>Start the FastAPI REST API server.</p> <pre><code>python main.py serve [OPTIONS]\n</code></pre> <p>When running inside Docker, the container starts in <code>serve</code> mode by default:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"cli-reference/#options_1","title":"Options","text":"Option Short Default Description <code>--host</code> <code>-h</code> <code>0.0.0.0</code> Network interface to bind to. Use <code>127.0.0.1</code> to restrict to localhost. <code>--port</code> <code>-p</code> <code>8000</code> Port number for the HTTP server. <code>--reload</code> <code>false</code> Enable auto-reload on file changes. Intended for development only. <code>--log-level</code> <code>INFO</code> Logging verbosity. Accepted values: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>."},{"location":"cli-reference/#examples_1","title":"Examples","text":"<p>Start the server on the default port:</p> <pre><code>python main.py serve\n</code></pre> <p>Start on a custom port with auto-reload:</p> <pre><code>python main.py serve --port 9000 --reload\n</code></pre>"},{"location":"cli-reference/#entrypoint-modes-docker","title":"Entrypoint Modes (Docker)","text":"<p>The Docker container uses an entrypoint script that accepts the following modes:</p> Mode Command Description <code>serve</code> <code>docker compose exec pipeline-api /entrypoint.sh serve</code> Start the API server (default) <code>crawl</code> <code>docker compose exec pipeline-api /entrypoint.sh crawl [OPTIONS]</code> Run a one-off crawl via the CLI <code>shell</code> <code>docker compose exec pipeline-api /entrypoint.sh shell</code> Open an interactive bash shell inside the container"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide covers installation, initial setup, and running your first crawl.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose (recommended), or</li> <li>Python 3.11+ for local execution</li> </ul>"},{"location":"getting-started/#docker-setup","title":"Docker Setup","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/PURVIK-31/website-crawler.git\ncd website-crawler\n</code></pre>"},{"location":"getting-started/#2-build-and-start-the-container","title":"2. Build and start the container","text":"<pre><code>docker compose up -d\n</code></pre> <p>This builds the image (first run takes a few minutes) and starts the API server on port <code>8000</code>. The container includes all dependencies, Playwright, and Chromium.</p>"},{"location":"getting-started/#3-verify-the-service-is-running","title":"3. Verify the service is running","text":"<pre><code>docker compose ps\n</code></pre> <p>The container should show a status of <code>Up</code> and <code>(healthy)</code>. You can also check the health endpoint:</p> <pre><code>curl http://localhost:8000/\n</code></pre> <p>Expected response:</p> <pre><code>{\n  \"service\": \"Website Ingestion Pipeline\",\n  \"status\": \"running\",\n  \"version\": \"1.0.0\",\n  \"docs\": \"/docs\"\n}\n</code></pre>"},{"location":"getting-started/#local-setup","title":"Local Setup","text":"<p>If you prefer running without Docker:</p>"},{"location":"getting-started/#1-create-a-virtual-environment","title":"1. Create a virtual environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate   # Linux/macOS\nvenv\\Scripts\\activate      # Windows\n</code></pre>"},{"location":"getting-started/#2-install-dependencies","title":"2. Install dependencies","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#3-install-playwright-browsers","title":"3. Install Playwright browsers","text":"<pre><code>playwright install chromium\n</code></pre> <p>Note</p> <p>Playwright is optional. If you skip this step, the pipeline will still work for static pages. Pass <code>--no-dynamic</code> to disable the Playwright fallback entirely.</p>"},{"location":"getting-started/#first-crawl","title":"First Crawl","text":""},{"location":"getting-started/#via-the-cli-docker","title":"Via the CLI (Docker)","text":"<pre><code>docker compose exec pipeline-api /entrypoint.sh crawl \\\n  --url https://example.com \\\n  --depth 2 \\\n  --limit 20 \\\n  --format parquet\n</code></pre>"},{"location":"getting-started/#via-the-cli-local","title":"Via the CLI (local)","text":"<pre><code>python main.py --url https://example.com --depth 2 --limit 20\n</code></pre>"},{"location":"getting-started/#via-the-api","title":"Via the API","text":"<pre><code>curl -X POST http://localhost:8000/api/crawl \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"url\": \"https://example.com\", \"max_depth\": 2, \"page_limit\": 20}'\n</code></pre> <p>This returns a job ID. Poll the status endpoint until the job completes:</p> <pre><code>curl http://localhost:8000/api/jobs/{job_id}\n</code></pre>"},{"location":"getting-started/#expected-output","title":"Expected Output","text":"<p>After a successful crawl, the output directory contains:</p> <pre><code>site_dataset/\n\u251c\u2500\u2500 pages.parquet\n\u251c\u2500\u2500 images.parquet\n\u251c\u2500\u2500 crawl_report.json\n\u251c\u2500\u2500 manifest.json\n\u251c\u2500\u2500 readable/\n\u2502   \u251c\u2500\u2500 all_pages.md\n\u2502   \u2514\u2500\u2500 example_com.md\n\u251c\u2500\u2500 raw_html/\n\u2502   \u2514\u2500\u2500 example.com/\n\u2514\u2500\u2500 images/\n    \u2514\u2500\u2500 example.com/\n</code></pre> <p>See Output Format for a full description of each file and its schema.</p>"},{"location":"getting-started/#stopping-the-service","title":"Stopping the Service","text":"<pre><code>docker compose down\n</code></pre> <p>Add <code>-v</code> to also remove the persistent data volume:</p> <pre><code>docker compose down -v\n</code></pre>"},{"location":"output-format/","title":"Output Format","text":"<p>Each crawl produces a self-contained output directory with structured datasets, raw archives, and human-readable exports.</p>"},{"location":"output-format/#directory-structure","title":"Directory Structure","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 pages.parquet              Structured page data\n\u251c\u2500\u2500 images.parquet             Image metadata\n\u251c\u2500\u2500 crawl_report.json          Crawl summary statistics\n\u251c\u2500\u2500 manifest.json              File listing with checksums\n\u251c\u2500\u2500 readable/\n\u2502   \u251c\u2500\u2500 all_pages.md           Combined readable export\n\u2502   \u2514\u2500\u2500 {page_slug}.md         Individual page exports\n\u251c\u2500\u2500 raw_html/\n\u2502   \u2514\u2500\u2500 {domain}/\n\u2502       \u251c\u2500\u2500 {hash}.html.gz     Gzip-compressed HTML\n\u2502       \u2514\u2500\u2500 {hash}.meta.json   Fetch metadata sidecar\n\u2514\u2500\u2500 images/\n    \u2514\u2500\u2500 {domain}/\n        \u2514\u2500\u2500 {hash}.{ext}       Downloaded images\n</code></pre> <p>The tabular files (<code>pages</code>, <code>images</code>) use the format specified at crawl time: <code>.parquet</code>, <code>.csv</code>, or <code>.jsonl</code>.</p>"},{"location":"output-format/#pages-dataset","title":"Pages Dataset","text":"<p>The <code>pages</code> file contains one row per successfully crawled page.</p> Field Type Description <code>url</code> string Canonical URL of the crawled page <code>title</code> string Page title extracted from <code>&lt;title&gt;</code> or the first <code>&lt;h1&gt;</code> <code>headings</code> string (JSON) JSON array of <code>{\"level\": int, \"text\": string}</code> objects for h1--h6 elements <code>content</code> string Cleaned body text with HTML tags, scripts, and styles removed <code>meta_description</code> string Content of the <code>&lt;meta name=\"description\"&gt;</code> or <code>og:description</code> tag <code>crawl_date</code> string (ISO 8601) UTC timestamp of when the page was crawled <code>word_count</code> integer Number of whitespace-delimited words in <code>content</code>"},{"location":"output-format/#images-dataset","title":"Images Dataset","text":"<p>The <code>images</code> file contains one row per image found across all crawled pages.</p> Field Type Description <code>image_path</code> string Local file path relative to the output directory (empty if downloading was disabled) <code>source_page</code> string URL of the page where the image was found <code>alt_text</code> string Alt text from the <code>&lt;img&gt;</code> tag <code>image_url</code> string Original absolute URL of the image <p>Images are deduplicated using perceptual hashing. Files are named by their SHA-256 content hash to avoid duplicates on disk.</p>"},{"location":"output-format/#crawl-report","title":"Crawl Report","text":"<p>The <code>crawl_report.json</code> file contains summary statistics for the crawl.</p> <pre><code>{\n  \"total_pages\": 15,\n  \"failed_pages\": 2,\n  \"total_images\": 42,\n  \"external_links\": 8,\n  \"time_taken_seconds\": 12.5,\n  \"errors\": [\n    {\n      \"url\": \"https://example.com/broken\",\n      \"error\": \"HTTP 404: Bad status\"\n    }\n  ]\n}\n</code></pre> Field Type Description <code>total_pages</code> integer Number of pages successfully crawled <code>failed_pages</code> integer Number of pages that failed (fetch, parse, or extraction errors) <code>total_images</code> integer Total images found across all pages <code>external_links</code> integer Count of unique external links discovered <code>time_taken_seconds</code> float Wall-clock time for the crawl in seconds <code>errors</code> array List of error records (capped at 100). Each contains <code>url</code> and <code>error</code>."},{"location":"output-format/#manifest","title":"Manifest","text":"<p>The <code>manifest.json</code> file lists every file in the output directory with its size and checksum.</p> <pre><code>{\n  \"created_at\": \"2026-02-23T10:01:30Z\",\n  \"total_files\": 8,\n  \"files\": [\n    {\n      \"path\": \"pages.parquet\",\n      \"size_bytes\": 12345,\n      \"sha256\": \"e3b0c44298fc1c14...\"\n    }\n  ]\n}\n</code></pre> <p>This file can be used to verify data integrity after transfer or archival.</p>"},{"location":"output-format/#readable-markdown","title":"Readable Markdown","text":"<p>The <code>readable/</code> directory contains human-readable Markdown renderings of each crawled page.</p> <ul> <li><code>{page_slug}.md</code> \u2014 Individual page. Includes title, URL, meta description, word count, page structure (headings), and cleaned body content.</li> <li><code>all_pages.md</code> \u2014 All pages combined into a single document, separated by horizontal rules.</li> </ul> <p>Page slugs are derived from the URL: the domain and path are converted to lowercase alphanumeric characters with underscores, truncated to 80 characters.</p>"},{"location":"output-format/#raw-html","title":"Raw HTML","text":"<p>The <code>raw_html/</code> directory stores the original HTML responses, organized by domain.</p> <ul> <li><code>{hash}.html.gz</code> \u2014 Gzip-compressed HTML. The hash is the SHA-256 of the response body, ensuring deduplication.</li> <li><code>{hash}.meta.json</code> \u2014 Sidecar file containing fetch metadata: URL, HTTP status code, response time, and content size.</li> </ul> <p>This directory is only created if raw HTML saving is enabled (the default). Disable it with <code>--no-raw-html</code> or <code>\"save_raw_html\": false</code>.</p>"},{"location":"output-format/#downloaded-images","title":"Downloaded Images","text":"<p>The <code>images/</code> directory contains downloaded image files, organized by source domain.</p> <ul> <li>Files are named by their SHA-256 content hash with the original file extension.</li> <li>Images smaller than 100px (configurable) are excluded.</li> <li>Perceptual hashing is used to detect and skip near-duplicate images.</li> </ul> <p>This directory is only created if image downloading is enabled (the default). Disable it with <code>--no-images</code> or <code>\"download_images\": false</code>.</p>"}]}